摘要：
最近的深度学习模型已经从低维规则网格(如图像、视频和语音)发展到高维图形结构数据(如社交网络、大脑连接和知识图)。这种演变导致了基于图形的不规则和稀疏模型的出现，超越了现有深度学习框架的设计目标。此外，这些模型不容易在并行硬件(如gpu)上实现高效、大规模、加速。
本文介绍了基于图的深度神经网络(GNNs)的第一个并行处理框架——NGra。NGra提出了一种新的SAGA-NN模型，将深度神经网络表示为顶点程序，每一层在明确的(Scatter, ApplyEdge, Gather, ApplyV ertex)图操作阶段。这个模型不仅允许直观地表达gnn，而且有助于映射到有效的数据流表示。NGra通过自动图形分区和GPU核心或多个GPU上的基于块的流处理来透明地解决可伸缩性挑战，它仔细考虑了数据局域性、数据移动以及并行处理和数据移动的重叠。NGra通过在gpu上高度优化的散射/聚集操作符进一步实现了效率，尽管它的稀疏性。我们的评估显示，NGra可以扩展到现有框架无法直接处理的大型真实图形，即使在小范围内，也比TensorFlow的多基线设计实现了大约4倍的加速。
介绍：
深度学习以深度神经网络(DNNs)的形式，因其具有良好的学习性能而受到广泛的关注，在语音、视觉和自然语言处理等领域的成功。在这些领域中，底层数据表示的坐标通常有一个规则的网格结构，这对拥有大量simd风格数据并行的硬件加速(例如GPU)很友好。有一个新兴的趋势在应用深度学习模型数据与一个不规则图形结构[9日,22日,25日,13日,15日,4日,5日,29),由图数据,如社交网络的重要性,知识图表和图表在生物信息学和神经科学(例如,蛋白质-蛋白质之间的关系或在大脑神经元连接),并将先进的预测结果在目标应用程序(如分类、嵌入和query-answering)。这些基于图的神经网络(GNNs)通常在与图中的顶点和边相关的特征上应用神经网络模型，并传播和聚合结果以产生下一级的特征。
现有的解决方案都不能很好地支持GNNs。现有的图处理引擎[28,26,11,6,41]通常提供一个类似于GAS的顶点程序模型，但不能在图结构中表达和支持神经网络体系结构。TensorFlow[3]、PyTorch[2]、MxNet[8]、CNTK[42]等深度学习框架的设计目的是将神经网络表示为数据流图，但不直接支持图传播模型。此外，它们都不提供处理大型图所需的可伸缩性，也不支持基于gpu的图传播算子(转换成稀疏操作)的高效实现。目前缺乏支持严重限制了大规模挖掘gnn全部潜力的能力，因为DNNs与大图结构的结合在系统层面上构成了重大挑战。
在本文中，我们提出首个支持大规模gnn的系统NGra，从一个易于表达的编程模型到一个可扩展且高效的gpu并行处理引擎。NGra在名为SAGA-NN (Scatter-ApplyEdgeGather-ApplyVertex with Neural Networks)的新模型中自然地将数据流与顶点程序抽象相结合。而传奇的可以被认为是一个变体气体模型、用户定义的函数SAGA-NN模型允许用户表达神经网络计算顶点或边数据(视为张量)通过使用抽象数据流,而不是那些专为传统图像处理(例如,如PageRank算法,连接组件,和最短路径)
与DNNs一样，gpu的有效使用对GNNs的性能至关重要，而且由于处理大型图结构的额外挑战，这种情况更加严重。实现可伸缩性超出了gpu的物理限制,NGra透明的分区图(顶点和边数据)成块,和GNN算法表示SAGA-NN模型转换为一个数据流图块粒度与运营商,通过它使chunk-based单个或多个gpu并行流处理
因此，NGra引擎的效率很大程度上取决于NGra如何管理和调度并行流处理，以及在gpu上实现关键的图形传播操作符(Scatter和Gather)。NGra非常注意数据的局部性，尽量减少数据进出GPU内存，最大化数据块在GPU内存中的重用，同时以流的方式重叠数据移动和计算。对于多gpu的情况，它使用了基于环的流机制，通过在gpu之间直接交换数据块来避免主机内存中的冗余数据移动。SAGA-NN模型中的散点和聚集阶段沿边缘进行顶点数据传播，表现为稀疏结构上的矩阵乘法。在GPU这样的数据并行硬件上执行稀疏矩阵运算是出了名的困难。因此，NGra在数据流图中引入了图形传播引擎支持的特殊运算符，并优化了数据流图在gpu上的执行。注意，与其他基于GPU的图形引擎关注的传统图形处理场景不同，在GNN场景中，可变顶点数据本身可能不能容纳在GPU设备内存中，因为每个顶点的数据可以是一个特征向量，而不是一个简单的标量。因此，我们的方案更倾向于利用逐顶点数据访问的并行性，以提高内存访问效率。
通过对TensorFlow的顶点程序抽象和图传播过程的自定义算子进行扩展，实现了NGra。我们证明NGra可以扩展到支持各种GNN算法在大型图表包含数以百万计的顶点与数以百计的特征维度和数百数以百万计的边缘通过利用单个服务器的主机内存和GPU的计算能力(s),不能直接通过使用现有的深度学习框架。与GPU支持的小图形TensorFlow相比，NGra可以获得约4倍的加速。我们还广泛评估了NGra中多个优化所带来的改进，以证明它们的有效性。
本文的其余部分组织如下。第2节介绍SAGA-NN编程抽象。第3节介绍了NGra系统中的组件、机制和优化。第4节介绍了NGra中基于环路的流方案，可扩展到多个gpu。第5节说明了SAGANN模型在应用程序中的用法。第六节讨论了NGra的实施和评价。我们在第7节讨论相关的工作，并在第8节结束。