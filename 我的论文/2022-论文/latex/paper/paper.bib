%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for aaron at 2022-05-23 21:47:48 +0800 


%% Saved with string encoding Unicode (UTF-8) 



@article{1,
	author = {Leen, G. and Heffernan, D.},
	date-added = {2022-05-23 21:47:03 +0800},
	date-modified = {2022-05-23 21:47:03 +0800},
	doi = {10.1109/2.976923},
	journal = {Computer},
	number = {1},
	pages = {88-93},
	title = {Expanding automotive electronic systems},
	volume = {35},
	year = {2002},
	Bdsk-Url-1 = {https://doi.org/10.1109/2.976923}}

@ARTICLE{2,
  author={Woo, Samuel and Jo, Hyo Jin and Lee, Dong Hoon},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={A Practical Wireless Attack on the Connected Car and Security Protocol for In-Vehicle CAN}, 
  year={2015},
  volume={16},
  number={2},
  pages={993-1006},
  doi={10.1109/TITS.2014.2351612}}

@INPROCEEDINGS{3,
  author={Tareq, Md. Mostofa Kamal and Semiari, Omid and Salehi, Mohsen Amini and Saad, Walid},
  booktitle={2018 IEEE Global Communications Conference (GLOBECOM)}, 
  title={Ultra Reliable, Low Latency Vehicle-to-Infrastructure Wireless Communications with Edge Computing}, 
  year={2018},
  volume={},
  number={},
  pages={1-7},
  doi={10.1109/GLOCOM.2018.8647367}}

@INPROCEEDINGS{4,
  author={Koscher, Karl and Czeskis, Alexei and Roesner, Franziska and Patel, Shwetak and Kohno, Tadayoshi and Checkoway, Stephen and McCoy, Damon and Kantor, Brian and Anderson, Danny and Shacham, Hovav and Savage, Stefan},
  booktitle={2010 IEEE Symposium on Security and Privacy}, 
  title={Experimental Security Analysis of a Modern Automobile}, 
  year={2010},
  volume={},
  number={},
  pages={447-462},
  doi={10.1109/SP.2010.34}}

@inproceedings {5,
	author = {Stephen Checkoway and Damon McCoy and Brian Kantor and Danny Anderson and Hovav Shacham and Stefan Savage and Karl Koscher and Alexei Czeskis and Franziska Roesner and Tadayoshi Kohno},
	title = {Comprehensive Experimental Analyses of Automotive Attack Surfaces},
	booktitle = {20th USENIX Security Symposium (USENIX Security 11)},
	year = {2011},
	address = {San Francisco, CA},
	url = {https://www.usenix.org/conference/usenix-security-11/comprehensive-experimental-analyses-automotive-attack-surfaces},
	publisher = {USENIX Association},
	month = aug,
}

@article{6,
  title={Adventures in automotive networks and control units},
  author={Miller, Charlie and Valasek, Chris},
  journal={Def Con},
  volume={21},
  number={260-264},
  pages={15--31},
  year={2013}
}

@misc{7,
  doi = {10.48550/ARXIV.1802.01725},
  url = {https://arxiv.org/abs/1802.01725},
  author = {Avatefipour, Omid and Malik, Hafiz}, 
  keywords = {Cryptography and Security (cs.CR), FOS: Computer and information sciences, FOS: Computer and information sciences}, 
  title = {State-of-the-Art Survey on In-Vehicle Network Communication (CAN-Bus) Security and Vulnerabilities},  
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@INPROCEEDINGS{8,
  author={Tashiro, Akiyoshi and Muraoka, Hideyuki and Araki, Shunsuke and Kakizaki, Ken'ichi and Uehara, Satoshi},
  booktitle={2017 3rd IEEE International Conference on Computer and Communications (ICCC)}, 
  title={A secure protocol consisting of two different security-level message authentications over CAN}, 
  year={2017},
  volume={},
  number={},
  pages={1520-1524},
  doi={10.1109/CompComm.2017.8322794}}

@INPROCEEDINGS{9,
  author={Nowdehi, Nasser and Lautenbach, Aljoscha and Olovsson, Tomas},
  booktitle={2017 IEEE 86th Vehicular Technology Conference (VTC-Fall)}, 
  title={In-Vehicle CAN Message Authentication: An Evaluation Based on Industrial Criteria}, 
  year={2017},
  volume={},
  number={},
  pages={1-7},
  doi={10.1109/VTCFall.2017.8288327}}

@ARTICLE{10,
  author={Choi, Wonsuk and Joo, Kyungho and Jo, Hyo Jin and Park, Moon Chan and Lee, Dong Hoon},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={VoltageIDS: Low-Level Communication Characteristics for Automotive Intrusion Detection System}, 
  year={2018},
  volume={13},
  number={8},
  pages={2114-2129},
  doi={10.1109/TIFS.2018.2812149}}

@article{11,
title = {In-vehicle network intrusion detection using deep convolutional neural network},
journal = {Vehicular Communications},
volume = {21},
pages = {100198},
year = {2020},
issn = {2214-2096},
doi = {https://doi.org/10.1016/j.vehcom.2019.100198},
url = {https://www.sciencedirect.com/science/article/pii/S2214209619302451},
author = {Hyun Min Song and Jiyoung Woo and Huy Kang Kim},
keywords = {In-vehicle network, Controller area network (CAN), Intrusion detection, Convolutional neural network (CNN)},
abstract = {The implementation of electronics in modern vehicles has resulted in an increase in attacks targeting in-vehicle networks; thus, attack detection models have caught the attention of the automotive industry and its researchers. Vehicle network security is an urgent and significant problem because the malfunctioning of vehicles can directly affect human and road safety. The controller area network (CAN), which is used as a de facto standard for in-vehicle networks, does not have sufficient security features, such as message encryption and sender authentication, to protect the network from cyber-attacks. In this paper, we propose an intrusion detection system (IDS) based on a deep convolutional neural network (DCNN) to protect the CAN bus of the vehicle. The DCNN learns the network traffic patterns and detects malicious traffic without hand-designed features. We designed the DCNN model, which was optimized for the data traffic of the CAN bus, to achieve high detection performance while reducing the unnecessary complexity in the architecture of the Inception-ResNet model. We performed an experimental study using the datasets we built with a real vehicle to evaluate our detection system. The experimental results demonstrate that the proposed IDS has significantly low false negative rates and error rates when compared to the conventional machine-learning algorithms.}
}

@INPROCEEDINGS{12,
  author={Taylor, Adrian and Leblanc, Sylvain and Japkowicz, Nathalie},
  booktitle={2016 IEEE International Conference on Data Science and Advanced Analytics (DSAA)}, 
  title={Anomaly Detection in Automobile Control Network Data with Long Short-Term Memory Networks}, 
  year={2016},
  volume={},
  number={},
  pages={130-139},
  doi={10.1109/DSAA.2016.20}}

@misc{13,
  doi = {10.48550/ARXIV.2006.02903},
  url = {https://arxiv.org/abs/2006.02903},
  author = {Ren, Pengzhen and Xiao, Yun and Chang, Xiaojun and Huang, Po-Yao and Li, Zhihui and Chen, Xiaojiang and Wang, Xin},
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {A Comprehensive Survey of Neural Architecture Search: Challenges and Solutions},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@InProceedings{14,
  title = 	 {{NAS}-Bench-101: Towards Reproducible Neural Architecture Search},
  author =       {Ying, Chris and Klein, Aaron and Christiansen, Eric and Real, Esteban and Murphy, Kevin and Hutter, Frank},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {7105--7114},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/ying19a/ying19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/ying19a.html},
  abstract = 	 {Recent advances in neural architecture search (NAS) demand tremendous computational resources, which makes it difficult to reproduce experiments and imposes a barrier-to-entry to researchers without access to large-scale computation. We aim to ameliorate these problems by introducing NAS-Bench-101, the first public architecture dataset for NAS research. To build NAS-Bench-101, we carefully constructed a compact, yet expressive, search space, exploiting graph isomorphisms to identify 423k unique convolutional architectures. We trained and evaluated all of these architectures multiple times on CIFAR-10 and compiled the results into a large dataset of over 5 million trained models. This allows researchers to evaluate the quality of a diverse range of models in milliseconds by querying the pre-computed dataset. We demonstrate its utility by analyzing the dataset as a whole and by benchmarking a range of architecture optimization algorithms.}
}

@inproceedings{15,
  title={Yen Gary G},
  author={Yanan, Sun and Bing, Xue and Mengjie, Zhang},
  booktitle={An experimental study on hyper-parameter optimization for stacked auto-encoders. In 2018 IEEE Congress on Evolutionary Computation (CEC)},
  pages={1--8},
  year={2019}
}

@INPROCEEDINGS{16,
  author={Ye, Furong and Doerr, Carola and Bäck, Thomas},
  booktitle={2019 IEEE Congress on Evolutionary Computation (CEC)}, 
  title={Interpolating Local and Global Search by Controlling the Variance of Standard Bit Mutation}, 
  year={2019},
  volume={},
  number={},
  pages={2292-2299},
  doi={10.1109/CEC.2019.8790107}}

@article{17, title={Regularized Evolution for Image Classifier Architecture Search}, volume={33}, url={https://ojs.aaai.org/index.php/AAAI/article/view/4405}, DOI={10.1609/aaai.v33i01.33014780}, abstractNote={&lt;p&gt;The effort devoted to hand-crafting neural network image classifiers has motivated the use of architecture search to discover them automatically. Although evolutionary algorithms have been repeatedly applied to neural network topologies, the image classifiers thus discovered have remained inferior to human-crafted ones. Here, we evolve an image classifier— &lt;em&gt;AmoebaNet-A&lt;/em&gt;—that surpasses hand-designs for the first time. To do this, we modify the tournament selection evolutionary algorithm by introducing an age property to favor the younger genotypes. Matching size, AmoebaNet-A has comparable accuracy to current state-of-the-art ImageNet models discovered with more complex architecture-search methods. Scaled to larger size, AmoebaNet-A sets a new state-of-theart 83.9% top-1 / 96.6% top-5 ImageNet accuracy. In a controlled comparison against a well known reinforcement learning algorithm, we give evidence that evolution can obtain results faster with the same hardware, especially at the earlier stages of the search. This is relevant when fewer compute resources are available. Evolution is, thus, a simple method to effectively discover high-quality architectures.&lt;/p&gt;}, number={01}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V.}, year={2019}, month={Jul.}, pages={4780-4789} }

@ARTICLE{18,
  author={Wang, Chaoyue and Xu, Chang and Yao, Xin and Tao, Dacheng},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={Evolutionary Generative Adversarial Networks}, 
  year={2019},
  volume={23},
  number={6},
  pages={921-934},
  doi={10.1109/TEVC.2019.2895748}}

@INPROCEEDINGS{19,
  author={Hamdioui, Said and Kvatinsky, Shahar and Cauwenberghs, Gert and Xie, Lei and Wald, Nimrod and Joshi, Siddharth and Elsayed, Hesham Mostafa and Corporaal, Henk and Bertels, Koen},
  booktitle={Design, Automation   Test in Europe Conference   Exhibition (DATE), 2017}, 
  title={Memristor for computing: Myth or reality?}, 
  year={2017},
  volume={},
  number={},
  pages={722-731},
  doi={10.23919/DATE.2017.7927083}}

@misc{20,
  doi = {10.48550/ARXIV.1804.09081},
  url = {https://arxiv.org/abs/1804.09081},
  author = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Efficient Multi-objective Neural Architecture Search via Lamarckian Evolution},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inbook{21,
author = {Tang, Chuzhe and Wang, Youyun and Dong, Zhiyuan and Hu, Gansen and Wang, Zhaoguo and Wang, Minjie and Chen, Haibo},
title = {XIndex: A Scalable Learned Index for Multicore Data Storage},
year = {2020},
isbn = {9781450368186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332466.3374547},
abstract = {We present XIndex, a concurrent ordered index designed for fast queries. Similar to a recent proposal of the learned index, XIndex uses learned models to optimize index efficiency. Comparing with the learned index, XIndex is able to effectively handle concurrent writes without affecting the query performance by leveraging fine-grained synchronization and a new compaction scheme, Two-Phase Compaction. Furthermore, XIndex adapts its structure according to run-time workload characteristics to support dynamic workload. We demonstrate the advantages of XIndex with both YCSB and TPC-C (KV), a TPC-C variant for key-value stores. XIndex achieves up to 3.2X and 4.4X performance improvement comparing with Masstree and Wormhole, respectively, on a 24-core machine, and it is open-sourced1.},
booktitle = {Proceedings of the 25th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {308–320},
numpages = {13}
}

@ARTICLE{22,
  author={Sun, Yanan and Xue, Bing and Zhang, Mengjie and Yen, Gary G. and Lv, Jiancheng},
  journal={IEEE Transactions on Cybernetics}, 
  title={Automatically Designing CNN Architectures Using the Genetic Algorithm for Image Classification}, 
  year={2020},
  volume={50},
  number={9},
  pages={3840-3854},
  doi={10.1109/TCYB.2020.2983860}}

@InProceedings{23,
author = {Xie, Lingxi and Yuille, Alan},
title = {Genetic CNN},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {Oct},
year = {2017}
}


@InProceedings{24,
  title = 	 {Large-Scale Evolution of Image Classifiers},
  author =       {Esteban Real and Sherry Moore and Andrew Selle and Saurabh Saxena and Yutaka Leon Suematsu and Jie Tan and Quoc V. Le and Alexey Kurakin},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {2902--2911},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/real17a/real17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/real17a.html},
  abstract = 	 {Neural networks have proven effective at solving difficult problems but designing their architectures can be challenging, even for image classification problems alone. Our goal is to minimize human participation, so we employ evolutionary algorithms to discover such networks automatically. Despite significant computational requirements, we show that it is now possible to evolve models with accuracies within the range of those published in the last year. Specifically, we employ simple evolutionary techniques at unprecedented scales to discover models for the CIFAR-10 and CIFAR-100 datasets, starting from trivial initial conditions and reaching accuracies of 94.6\% (95.6\% for ensemble) and 77.0\%, respectively. To do this, we use novel and intuitive mutation operators that navigate large search spaces; we stress that no human participation is required once evolution starts and that the output is a fully-trained model. Throughout this work, we place special emphasis on the repeatability of results, the variability in the outcomes and the computational requirements.}
}

@InProceedings{25,
author="Lu, Zhichao
and Deb, Kalyanmoy
and Goodman, Erik
and Banzhaf, Wolfgang
and Boddeti, Vishnu Naresh",
editor="Vedaldi, Andrea
and Bischof, Horst
and Brox, Thomas
and Frahm, Jan-Michael",
title="NSGANetV2: Evolutionary Multi-objective Surrogate-Assisted Neural Architecture Search",
booktitle="Computer Vision -- ECCV 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="35--51",
abstract="In this paper, we propose an efficient NAS algorithm for generating task-specific models that are competitive under multiple competing objectives. It comprises of two surrogates, one at the architecture level to improve sample efficiency and one at the weights level, through a supernet, to improve gradient descent training efficiency. On standard benchmark datasets (C10, C100, ImageNet), the resulting models, dubbed NSGANetV2, either match or outperform models from existing approaches with the search being orders of magnitude more sample efficient. Furthermore, we demonstrate the effectiveness and versatility of the proposed method on six diverse non-standard datasets, e.g. STL-10, Flowers102, Oxford Pets, FGVC Aircrafts etc. In all cases, NSGANetV2s improve the state-of-the-art (under mobile setting), suggesting that NAS can be a viable alternative to conventional transfer learning approaches in handling diverse scenarios such as small-scale or fine-grained datasets. Code is available at https://github.com/mikelzc1990/nsganetv2.",
isbn="978-3-030-58452-8"
}

@misc{26,
  doi = {10.48550/ARXIV.1705.10823},
  url = {https://arxiv.org/abs/1705.10823},
  author = {Baker, Bowen and Gupta, Otkrist and Raskar, Ramesh and Naik, Nikhil},
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Accelerating Neural Architecture Search using Performance Prediction},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@InProceedings{27,
author = {Liu, Chenxi and Zoph, Barret and Neumann, Maxim and Shlens, Jonathon and Hua, Wei and Li, Li-Jia and Fei-Fei, Li and Yuille, Alan and Huang, Jonathan and Murphy, Kevin},
title = {Progressive Neural Architecture Search},
booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
month = {September},
year = {2018}
}

@misc{28,
  doi = {10.48550/ARXIV.1908.09791},
  url = {https://arxiv.org/abs/1908.09791},
  author = {Cai, Han and Gan, Chuang and Wang, Tianzhe and Zhang, Zhekai and Han, Song},
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Once-for-All: Train One Network and Specialize it for Efficient Deployment},
  publisher = {arXiv},
  year = {2019},
  copyright = {Creative Commons Attribution 4.0 International}
}

@InProceedings{29,
author = {Dai, Xiaoliang and Zhang, Peizhao and Wu, Bichen and Yin, Hongxu and Sun, Fei and Wang, Yanghan and Dukhan, Marat and Hu, Yunqing and Wu, Yiming and Jia, Yangqing and Vajda, Peter and Uyttendaele, Matt and Jha, Niraj K.},
title = {ChamNet: Towards Efficient Network Design Through Platform-Aware Model Adaptation},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}


@InProceedings{30,
  title = 	 {Addressing Function Approximation Error in Actor-Critic Methods},
  author =       {Fujimoto, Scott and van Hoof, Herke and Meger, David},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1587--1596},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/fujimoto18a/fujimoto18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/fujimoto18a.html},
  abstract = 	 {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.}
}

@data{31,
doi = {10.21227/qvr7-n418},
url = {https://dx.doi.org/10.21227/qvr7-n418},
author = {Kang, Hyunjae and Kwak, Byung Il and Lee, Young Hun and Lee, Haneol and Lee, Hwejae and Kim, Huy Kang},
publisher = {IEEE Dataport},
title = {Car Hacking: Attack $\&$ Defense Challenge 2020 Dataset},
year = {2021} }

@inproceedings{32,
author = {Ma, Yao and Wang, Suhang and Aggarwal, Charu C. and Tang, Jiliang},
title = {Graph Convolutional Networks with EigenPooling},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330982},
doi = {10.1145/3292500.3330982},
abstract = {Graph neural networks, which generalize deep neural network models to graph structured data, have attracted increasing attention in recent years. They usually learn node representations by transforming, propagating and aggregating node features and have been proven to improve the performance of many graph related tasks such as node classification and link prediction. To apply graph neural networks for the graph classification task, approaches to generate thegraph representation from node representations are demanded. A common way is to globally combine the node representations. However, rich structural information is overlooked. Thus a hierarchical pooling procedure is desired to preserve the graph structure during the graph representation learning. There are some recent works on hierarchically learning graph representation analogous to the pooling step in conventional convolutional neural (CNN) networks. However, the local structural information is still largely neglected during the pooling process. In this paper, we introduce a pooling operator $pooling$ based on graph Fourier transform, which can utilize the node features and local structures during the pooling process. We then design pooling layers based on the pooling operator, which are further combined with traditional GCN convolutional layers to form a graph neural network framework $m$ for graph classification. Theoretical analysis is provided to understand $pooling$ from both local and global perspectives. Experimental results of the graph classification task on $6$ commonly used benchmarks demonstrate the effectiveness of the proposed framework.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery $\&$amp; Data Mining},
pages = {723–731},
numpages = {9},
keywords = {graph convolution networks, pooling, spectral graph theory, graph classification},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@InProceedings{33,
author = {Yu, Jiahui and Huang, Thomas S.},
title = {Universally Slimmable Networks and Improved Training Techniques},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019}
}

@misc{34,
  doi = {10.48550/ARXIV.1611.01578},
  url = {https://arxiv.org/abs/1611.01578},
  author = {Zoph, Barret and Le, Quoc V.},
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Neural Architecture Search with Reinforcement Learning},
  publisher = {arXiv},
  year = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{35,
  doi = {10.48550/ARXIV.1912.12814},
  url = {https://arxiv.org/abs/1912.12814},
  author = {Jin, Xiaojie and Wang, Jiang and Slocum, Joshua and Yang, Ming-Hsuan and Dai, Shengyang and Yan, Shuicheng and Feng, Jiashi},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {RC-DARTS: Resource Constrained Differentiable Architecture Search},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@InProceedings{36,
author = {Zheng, Xiawu and Ji, Rongrong and Tang, Lang and Zhang, Baochang and Liu, Jianzhuang and Tian, Qi},
title = {Multinomial Distribution Learning for Effective Neural Architecture Search},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019}
}

@InProceedings{37,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
title = {Deep Residual Learning for Image Recognition},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2016}
}

@inproceedings{38,
  title={Inception-v4, inception-resnet and the impact of residual connections on learning},
  author={Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A},
  booktitle={Thirty-first AAAI conference on artificial intelligence},
  year={2017}
}

@InProceedings{39,
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
title = {Rethinking the Inception Architecture for Computer Vision},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2016}
}

@INPROCEEDINGS{40,
  author={Kang, Min-Ju and Kang, Je-Won},
  booktitle={2016 IEEE 83rd Vehicular Technology Conference (VTC Spring)}, 
  title={A Novel Intrusion Detection Method Using Deep Neural Network for In-Vehicle Network Security}, 
  year={2016},
  volume={},
  number={},
  pages={1-5},
  doi={10.1109/VTCSpring.2016.7504089}}