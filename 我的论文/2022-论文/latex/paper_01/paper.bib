%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for aaron at 2022-05-23 21:47:48 +0800 


%% Saved with string encoding Unicode (UTF-8) 



@INPROCEEDINGS{1,
  author={Prasad, Adithi and Shanthi, P.},
  booktitle={2021 Asian Conference on Innovation in Technology (ASIANCON)}, 
  title={Automotive Electronic Control Unit Reprogramming Using Delta Method-A Review}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/ASIANCON51346.2021.9544595}}


@Article{2,
AUTHOR = {Zhang, Haichun and Meng, Xu and Zhang, Xiong and Liu, Zhenglin},
TITLE = {CANsec: A Practical in-Vehicle Controller Area Network Security Evaluation Tool},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {17},
ARTICLE-NUMBER = {4900},
URL = {https://www.mdpi.com/1424-8220/20/17/4900},
ISSN = {1424-8220},
ABSTRACT = {The Internet of Things (IoT) is an industry-recognized next intelligent life solution that increases the level of comfort, efficiency, and automation for citizens through numerous sensors, smart devices, and cloud stations connected physically. As an important application scenario of IoT, the Internet of Vehicles (IoV) plays an extremely critical role in the intelligent transportation field. In fact, the In-Vehicle Network of smart vehicles that are recognized as the core roles in intelligent transportation is currently the Controller Area Network (CAN). However, the In-Vehicle CAN bus protocol has several vulnerabilities without any encryption, authentication, or integrity checking, which severely threatens the safety of drivers and passengers. Once malicious attackers hack the vehicular gateway and obtain the access right of the CAN, they may control the vehicle based on the vulnerabilities of the CAN bus protocol. Given the severe security risk of CAN, we proposed the CANsec, a practical In-Vehicle CAN security evaluation tool that simulates malicious attacks according to major attack models to evaluate the security risk of the In-Vehicle CAN. We also show a usage case of the CANsec without knowing any information from the vehicle manufacturer.},
DOI = {10.3390/s20174900}
}


@INPROCEEDINGS{3,
  author={Tareq, Md. Mostofa Kamal and Semiari, Omid and Salehi, Mohsen Amini and Saad, Walid},
  booktitle={2018 IEEE Global Communications Conference (GLOBECOM)}, 
  title={Ultra Reliable, Low Latency Vehicle-to-Infrastructure Wireless Communications with Edge Computing}, 
  year={2018},
  volume={},
  number={},
  pages={1-7},
  doi={10.1109/GLOCOM.2018.8647367}}

@article{KWAK2021114066,
title = {Cosine similarity based anomaly detection methodology for the CAN bus},
journal = {Expert Systems with Applications},
volume = {166},
pages = {114066},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.114066},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420308289},
author = {Byung Il Kwak and Mee Lan Han and Huy Kang Kim},
keywords = {In-vehicle network, Anomaly detection, Cosine similarity, Self-similarity},
abstract = {In recent years, vehicular technology has rapidly evolved in terms of the driver’s convenience and safety, along with the convergence of vehicle communication and the expansion of external interfaces. However, the connectivity of the vehicle to the external environment poses a considerable driving risk because of the pre-existing vulnerabilities in the vehicle. Furthermore, most of the in-vehicle networks, such as controller area network (CAN), local interconnect network (LIN), and FlexRay network, are not ready to cope with malicious attacks from the outside. For that reason, various studies have addressed the security issues of the automobiles, as protecting the life and safety of the drivers and passengers is one of the core values of the in-vehicle technology. In the present study, in order to address these critical security issues, we propose an anomaly detection method based on cosine similarity for in-vehicle network through the analysis of self-similarity of the CAN bus. Our main goal is to detect three types of injection attacks without having additional information about the attacks. To this end, we evaluated the performance of the proposed method by measuring the accuracy and detection time using a dataset extracted from two real vehicles in driving and stationary conditions. More specifically, we designed a light-weight feature vector that can accomplish real-time detection and then analyzed the performance in terms of accuracy, recall, and detection time by the time window. In the performance evaluation, we achieved high detection accuracy–namely, 98.93% and 99.18% for KIA Soul in the driving condition and in the stationary condition, respectively, 99.43% and 99.49% for the HYUNDAI YF Sonata in the driving condition and in the stationary condition, respectively. Finally, we also showed that the cosine similarity in the CAN bus is a meaningful feature to identify and classify the types of attacks on target CAN IDs.}
}



@ARTICLE{5,
  author={Hanselmann, Markus and Strauss, Thilo and Dormann, Katharina and Ulmer, Holger},
  journal={IEEE Access}, 
  title={CANet: An Unsupervised Intrusion Detection System for High Dimensional CAN Bus Data}, 
  year={2020},
  volume={8},
  number={},
  pages={58194-58205},
  doi={10.1109/ACCESS.2020.2982544}}


@Article{6,
AUTHOR = {Karopoulos, Georgios and Kambourakis, Georgios and Chatzoglou, Efstratios and Hernández-Ramos, José L. and Kouliaridis, Vasileios},
TITLE = {Demystifying In-Vehicle Intrusion Detection Systems: A Survey of Surveys and a Meta-Taxonomy},
JOURNAL = {Electronics},
VOLUME = {11},
YEAR = {2022},
NUMBER = {7},
ARTICLE-NUMBER = {1072},
URL = {https://www.mdpi.com/2079-9292/11/7/1072},
ISSN = {2079-9292},
ABSTRACT = {Breaches in the cyberspace due to cyber-physical attacks can harm the physical space, and any type of vehicle is an alluring target for wrongdoers for an assortment of reasons. Especially, as the automobiles are becoming increasingly interconnected within the Cooperative Intelligent Transport System (C-ITS) realm and their level of automation elevates, the risk for cyberattacks augments along with the attack surface, thus inexorably rendering the risk of complacency and inaction sizable. Next to other defensive measures, intrusion detection systems (IDS) already comprise an inextricable component of modern automobiles in charge of detecting intrusions in the system while in operation. This work concentrates on in-vehicle IDS with the goal to deliver a fourfold comprehensive survey of surveys on this topic. First, we collect and analyze all existing in-vehicle IDS classifications and fuse them into a simpler, overarching one that can be used as a base for classifying any work in this area. Second, we gather and elaborate on the so-far available datasets which can be possibly used to train and evaluate an in-vehicle IDS. Third, we survey non-commercial simulators which may be utilized for creating a dataset or evaluating an IDS. The last contribution pertains to a thorough exposition of the future trends and challenges in this area. To our knowledge, this work provides the first wholemeal survey on in-vehicle IDS, and it is therefore anticipated to serve as a groundwork and point of reference for multiple stakeholders at varying levels.},
DOI = {10.3390/electronics11071072}
}


@ARTICLE{9439954,
  author={Jo, Hyo Jin and Choi, Wonsuk},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={A Survey of Attacks on Controller Area Networks and Corresponding Countermeasures}, 
  year={2021},
  volume={},
  number={},
  pages={1-19},
  doi={10.1109/TITS.2021.3078740}}

@INPROCEEDINGS{9322395,
  author={Delwar Hossain, Md and Inoue, Hiroyuki and Ochiai, Hideya and Fall, Doudou and Kadobayashi, Youki},
  booktitle={GLOBECOM 2020 - 2020 IEEE Global Communications Conference}, 
  title={An Effective In-Vehicle CAN Bus Intrusion Detection System Using CNN Deep Learning Approach}, 
  year={2020},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/GLOBECOM42002.2020.9322395}}



@ARTICLE{7,
  author={Zhang, Daokun and Yin, Jie and Zhu, Xingquan and Zhang, Chengqi},
  journal={IEEE Transactions on Big Data}, 
  title={Network Representation Learning: A Survey}, 
  year={2020},
  volume={6},
  number={1},
  pages={3-28},
  doi={10.1109/TBDATA.2018.2850013}}


@ARTICLE{8,
  author={Cui, Peng and Wang, Xiao and Pei, Jian and Zhu, Wenwu},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={A Survey on Network Embedding}, 
  year={2019},
  volume={31},
  number={5},
  pages={833-852},
  doi={10.1109/TKDE.2018.2849727}}

@article{9,
author = {Wu, Shiwen and Sun, Fei and Zhang, Wentao and Xie, Xu and Cui, Bin},
title = {Graph Neural Networks in Recommender Systems: A Survey},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3535101},
doi = {10.1145/3535101},
abstract = {With the explosive growth of online information, recommender systems play a key role to alleviate such information overload. Due to the important application value of recommender systems, there have always been emerging works in this field. In recommender systems, the main challenge is to learn the effective user/item representations from their interactions and side information (if any). Recently, graph neural network (GNN) techniques have been widely utilized in recommender systems since most of the information in recommender systems essentially has graph structure and GNN has superiority in graph representation learning. This article aims to provide a comprehensive review of recent research efforts on GNN-based recommender systems. Specifically, we provide a taxonomy of GNN-based recommendation models according to the types of information used and recommendation tasks. Moreover, we systematically analyze the challenges of applying GNN on different types of data and discuss how existing works in this field address these challenges. Furthermore, we state new perspectives pertaining to the development of this field. We collect the representative papers along with their open-source implementations in https://github.com/wusw14/GNN-in-RS.},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = {mar},
keywords = {Recommender System; Graph Neural Network; Survey}
}

@inproceedings{10,
author = {Xie, Qianqian and Huang, Jimin and Du, Pan and Peng, Min and Nie, Jian-Yun},
title = {Graph Topic Neural Network for Document Representation},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3450045},
doi = {10.1145/3442381.3450045},
abstract = { Graph Neural Networks (GNNs) such as GCN can effectively learn document representations via the semantic relation graph among documents and words. However, despite a few exceptions, most of the previous work in this line of research does not consider the underlying topical semantics inherited in document contents and the relation graph, making the representations less effective and hard to interpret. In a few recent studies trying to incorporate latent topics into GNNs, the topics have been learned independently from the relation graph modeling. Intuitively, topic extraction can benefit much from the information propagation of the relation graph structure - directly and indirectly connected documents and words have similar topics. In this paper, we propose a novel Graph Topic Neural Network (GTNN) model to mine latent topic semantics for interpretable document representation learning, taking into account the document-document, document-word, and word-word relationships in the graph. We also show that our model can be viewed as semi-amortized inference for relational topic model based on Poisson distribution, with high order correlations. We test our model in several settings: unsupervised, semi-supervised, and supervised representation learning, for both connected and unconnected documents. In all the cases, our model outperforms the state-of-the-art models for these tasks.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {3055–3065},
numpages = {11},
keywords = {graph neural networks, topic models, document representation},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@ARTICLE{11,
AUTHOR={Attarilar, Shokouh and Yang, Jinfan and Ebrahimi, Mahmoud and Wang, Qingge and Liu, Jia and Tang, Yujin and Yang, Junlin},   	 
TITLE={The Toxicity Phenomenon and the Related Occurrence in Metal and Metal Oxide Nanoparticles: A Brief Review From the Biomedical Perspective},      	
JOURNAL={Frontiers in Bioengineering and Biotechnology},      	
VOLUME={8},      	
YEAR={2020},      	  
URL={https://www.frontiersin.org/article/10.3389/fbioe.2020.00822},       	
DOI={10.3389/fbioe.2020.00822},      	
ISSN={2296-4185},
ABSTRACT={Thousands of different nanoparticles (NPs) involve in our daily life with various origins from food, cosmetics, drugs, etc. It is believed that decreasing the size of materials up to nanometer levels can facilitate their unfavorable absorption since they can pass the natural barriers of live tissues and organs even, they can go across the relatively impermeable membranes. The interaction of these NPs with the biological environment disturbs the natural functions of cells and its components and cause health issues. In the lack of the detailed and comprehensive standard protocols about the toxicity of NPs materials, their control, and effects, this review study focuses on the current research literature about the related factors in toxicity of NPs such as size, concentration, etc. with an emphasis on metal and metal oxide nanoparticles. The goal of the study is to highlight their potential hazard and the advancement of green non-cytotoxic nanomaterials with safe threshold dose levels to resolve the toxicity issues. This study supports the NPs design along with minimizing the adverse effects of nanoparticles especially those used in biological treatments.}
}

@article{12,
title = {Graph neural networks: A review of methods and applications},
journal = {AI Open},
volume = {1},
pages = {57-81},
year = {2020},
issn = {2666-6510},
doi = {https://doi.org/10.1016/j.aiopen.2021.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S2666651021000012},
author = {Jie Zhou and Ganqu Cui and Shengding Hu and Zhengyan Zhang and Cheng Yang and Zhiyuan Liu and Lifeng Wang and Changcheng Li and Maosong Sun},
keywords = {Deep learning, Graph neural network},
abstract = {Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics systems, learning molecular fingerprints, predicting protein interface, and classifying diseases demand a model to learn from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures (like the dependency trees of sentences and the scene graphs of images) is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs) are neural models that capture the dependence of graphs via message passing between the nodes of graphs. In recent years, variants of GNNs such as graph convolutional network (GCN), graph attention network (GAT), graph recurrent network (GRN) have demonstrated ground-breaking performances on many deep learning tasks. In this survey, we propose a general design pipeline for GNN models and discuss the variants of each component, systematically categorize the applications, and propose four open problems for future research.}
}

@article{13,
title = {Learning label correlations for multi-label image recognition with graph networks},
journal = {Pattern Recognition Letters},
volume = {138},
pages = {378-384},
year = {2020},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2020.07.040},
url = {https://www.sciencedirect.com/science/article/pii/S0167865520302968},
author = {Qing Li and Xiaojiang Peng and Yu Qiao and Qiang Peng},
keywords = {Multi-label image recognition, Graph convolutional networks, Label correlation graph, Sparse correlation constraint},
abstract = {Multi-label image recognition is a task that predicts a set of object labels in an image. As the objects co-occur in the physical world, it is desirable to model label dependencies. Previous existing methods resort to either recurrent networks or pre-defined label correlation graphs for this purpose. In this paper, instead of using a pre-defined graph which is inflexible and may be sub-optimal for multi-label classification, we propose the A-GCN, which leverages the popular Graph Convolutional Networks with an Adaptive label correlation graph to model label dependencies. Specifically, we introduce a plug-and-play Label Graph (LG) module to learn label correlations with word embeddings, and then utilize traditional GCN to map this graph into label-dependent object classifiers which are further applied to image features. The basic LG module incorporates two 1 × 1 convolutional layers and uses the dot product to generate label graphs. In addition, we propose a sparse correlation constraint to enhance the LG module, and also explore different LG architectures. We validate our method on two diverse multi-label datasets: MS-COCO and Fashion550K. Experimental results show that our A-GCN significantly improves baseline methods and achieves performance superior or comparable to the state of the art.}
}

@article{14,
    author = {Sun, Mengying and Zhao, Sendong and Gilvary, Coryandar and Elemento, Olivier and Zhou, Jiayu and Wang, Fei},
    title = "{Graph convolutional networks for computational drug development and discovery}",
    journal = {Briefings in Bioinformatics},
    volume = {21},
    number = {3},
    pages = {919-935},
    year = {2019},
    month = {06},
    abstract = "{Despite the fact that deep learning has achieved remarkable success in various domains over the past decade, its application in molecular informatics and drug discovery is still limited. Recent advances in adapting deep architectures to structured data have opened a new paradigm for pharmaceutical research. In this survey, we provide a systematic review on the emerging field of graph convolutional networks and their applications in drug discovery and molecular informatics. Typically we are interested in why and how graph convolution networks can help in drug-related tasks. We elaborate the existing applications through four perspectives: molecular property and activity prediction, interaction prediction, synthesis prediction and de novo drug design. We briefly introduce the theoretical foundations behind graph convolutional networks and illustrate various architectures based on different formulations. Then we summarize the representative applications in drug-related problems. We also discuss the current challenges and future possibilities of applying graph convolutional networks to drug discovery.}",
    issn = {1477-4054},
    doi = {10.1093/bib/bbz042},
    url = {https://doi.org/10.1093/bib/bbz042},
    eprint = {https://academic.oup.com/bib/article-pdf/21/3/919/33227266/bbz042.pdf},
}

@article{15, title={Attention Based Spatial-Temporal Graph Convolutional Networks for Traffic Flow Forecasting}, volume={33}, url={https://ojs.aaai.org/index.php/AAAI/article/view/3881}, DOI={10.1609/aaai.v33i01.3301922}, abstractNote={&lt;p&gt;Forecasting the traffic flows is a critical issue for researchers and practitioners in the field of transportation. However, it is very challenging since the traffic flows usually show high nonlinearities and complex patterns. Most existing traffic flow prediction methods, lacking abilities of modeling the dynamic spatial-temporal correlations of traffic data, thus cannot yield satisfactory prediction results. In this paper, we propose a novel attention based spatial-temporal graph convolutional network (ASTGCN) model to solve traffic flow forecasting problem. ASTGCN mainly consists of three independent components to respectively model three temporal properties of traffic flows, i.e., recent, daily-periodic and weekly-periodic dependencies. More specifically, each component contains two major parts: 1) the spatial-temporal attention mechanism to effectively capture the dynamic spatialtemporal correlations in traffic data; 2) the spatial-temporal convolution which simultaneously employs graph convolutions to capture the spatial patterns and common standard convolutions to describe the temporal features. The output of the three components are weighted fused to generate the final prediction results. Experiments on two real-world datasets from the Caltrans Performance Measurement System (PeMS) demonstrate that the proposed ASTGCN model outperforms the state-of-the-art baselines.&lt;/p&gt;}, number={01}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Guo, Shengnan and Lin, Youfang and Feng, Ning and Song, Chao and Wan, Huaiyu}, year={2019}, month={Jul.}, pages={922-929} }


@ARTICLE{16,
  author={Skarding, Joakim and Gabrys, Bogdan and Musial, Katarzyna},
  journal={IEEE Access}, 
  title={Foundations and Modeling of Dynamic Networks Using Dynamic Graph Neural Networks: A Survey}, 
  year={2021},
  volume={9},
  number={},
  pages={79143-79168},
  doi={10.1109/ACCESS.2021.3082932}}

@inproceedings{17,
author = {Taheri, Aynaz and Gimpel, Kevin and Berger-Wolf, Tanya},
title = {Learning to Represent the Evolution of Dynamic Graphs with Recurrent Models},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308560.3316581},
doi = {10.1145/3308560.3316581},
abstract = {Graph representation learning for static graphs is a well studied topic. Recently, a few studies have focused on learning temporal information in addition to the topology of a graph. Most of these studies have relied on learning to represent nodes and substructures in dynamic graphs. However, the representation learning problem for entire graphs in a dynamic context is yet to be addressed. In this paper, we propose an unsupervised representation learning architecture for dynamic graphs, designed to learn both the topological and temporal features of the graphs that evolve over time. The approach consists of a sequence-to-sequence encoder-decoder model embedded with gated graph neural networks (GGNNs) and long short-term memory networks (LSTMs). The GGNN is able to learn the topology of the graph at each time step, while LSTMs are leveraged to propagate the temporal information among the time steps. Moreover, an encoder learns the temporal dynamics of an evolving graph and a decoder reconstructs the dynamics over the same period of time using the encoded representation provided by the encoder. We demonstrate that our approach is capable of learning the representation of a dynamic graph through time by applying the embeddings to dynamic graph classification using a real world dataset of animal behaviour.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {301–307},
numpages = {7},
keywords = {dynamic graph, representation learning, recurrent models},
location = {San Francisco, USA},
series = {WWW '19}
}

@inbook{18,
author = {Wang, Xiaoyang and Ma, Yao and Wang, Yiqi and Jin, Wei and Wang, Xin and Tang, Jiliang and Jia, Caiyan and Yu, Jian},
title = {Traffic Flow Prediction via Spatial Temporal Graph Neural Network},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380186},
abstract = {Traffic flow analysis, prediction and management are keystones for building smart cities in the new era. With the help of deep neural networks and big traffic data, we can better understand the latent patterns hidden in the complex transportation networks. The dynamic of the traffic flow on one road not only depends on the sequential patterns in the temporal dimension but also relies on other roads in the spatial dimension. Although there are existing works on predicting the future traffic flow, the majority of them have certain limitations on modeling spatial and temporal dependencies. In this paper, we propose a novel spatial temporal graph neural network for traffic flow prediction, which can comprehensively capture spatial and temporal patterns. In particular, the framework offers a learnable positional attention mechanism to effectively aggregate information from adjacent roads. Meanwhile, it provides a sequential component to model the traffic flow dynamics which can exploit both local and global temporal dependencies. Experimental results on various real traffic datasets demonstrate the effectiveness of the proposed framework.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1082–1092},
numpages = {11}
}


@inproceedings{19,
author = {Ma, Yao and Wang, Suhang and Aggarwal, Charu C. and Tang, Jiliang},
title = {Graph Convolutional Networks with EigenPooling},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330982},
doi = {10.1145/3292500.3330982},
abstract = {Graph neural networks, which generalize deep neural network models to graph structured data, have attracted increasing attention in recent years. They usually learn node representations by transforming, propagating and aggregating node features and have been proven to improve the performance of many graph related tasks such as node classification and link prediction. To apply graph neural networks for the graph classification task, approaches to generate thegraph representation from node representations are demanded. A common way is to globally combine the node representations. However, rich structural information is overlooked. Thus a hierarchical pooling procedure is desired to preserve the graph structure during the graph representation learning. There are some recent works on hierarchically learning graph representation analogous to the pooling step in conventional convolutional neural (CNN) networks. However, the local structural information is still largely neglected during the pooling process. In this paper, we introduce a pooling operator $pooling$ based on graph Fourier transform, which can utilize the node features and local structures during the pooling process. We then design pooling layers based on the pooling operator, which are further combined with traditional GCN convolutional layers to form a graph neural network framework $m$ for graph classification. Theoretical analysis is provided to understand $pooling$ from both local and global perspectives. Experimental results of the graph classification task on $6$ commonly used benchmarks demonstrate the effectiveness of the proposed framework.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery $\&$amp; Data Mining},
pages = {723–731},
numpages = {9},
keywords = {graph convolution networks, pooling, spectral graph theory, graph classification},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@InProceedings{20,
  title = 	 {{BOHB}: Robust and Efficient Hyperparameter Optimization at Scale},
  author =       {Falkner, Stefan and Klein, Aaron and Hutter, Frank},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1437--1446},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/falkner18a/falkner18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/falkner18a.html},
  abstract = 	 {Modern deep learning methods are very sensitive to many hyperparameters, and, due to the long training times of state-of-the-art models, vanilla Bayesian hyperparameter optimization is typically computationally infeasible. On the other hand, bandit-based configuration evaluation approaches based on random search lack guidance and do not converge to the best configurations as quickly. Here, we propose to combine the benefits of both Bayesian optimization and bandit-based methods, in order to achieve the best of both worlds: strong anytime performance and fast convergence to optimal configurations. We propose a new practical state-of-the-art hyperparameter optimization method, which consistently outperforms both Bayesian optimization and Hyperband on a wide range of problem types, including high-dimensional toy functions, support vector machines, feed-forward neural networks, Bayesian neural networks, deep reinforcement learning, and convolutional neural networks. Our method is robust and versatile, while at the same time being conceptually simple and easy to implement.}
}

@misc{21,
  doi = {10.48550/ARXIV.2101.09300},
  url = {https://arxiv.org/abs/2101.09300},
  author = {Yuan, Yingfang and Wang, Wenjun and Coghill, George M. and Pang, Wei},
  keywords = {Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {A Novel Genetic Algorithm with Hierarchical Evaluation Strategy for Hyperparameter Optimisation of Graph Neural Networks},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{22,
title = {Reinforcement learning for neural architecture search: A review},
journal = {Image and Vision Computing},
volume = {89},
pages = {57-66},
year = {2019},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2019.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S0262885619300885},
author = {Yesmina Jaafra and Jean {Luc Laurent} and Aline Deruyver and Mohamed {Saber Naceur}},
keywords = {Reinforcement learning, Convolutional neural networks, Neural Architecture Search, AutoML},
abstract = {Deep neural networks are efficient and flexible models that perform well for a variety of tasks such as image, speech recognition and natural language understanding. In particular, convolutional neural networks (CNN) generate a keen interest among researchers in computer vision and more specifically in classification tasks. CNN architecture and related hyperparameters are generally correlated to the nature of the processed task as the network extracts complex and relevant characteristics allowing the optimal convergence. Designing such architectures requires significant human expertise, substantial computation time and does not always lead to the optimal network. Reinforcement learning (RL) has been extensively used in automating CNN models design generating notable advances and interesting results in the field. This work aims at reviewing and discussing the recent progress of RL methods in Neural Architecture Search task and the current challenges that still require further consideration.}
}

@ARTICLE{23,
  author={Liu, Yuqiao and Sun, Yanan and Xue, Bing and Zhang, Mengjie and Yen, Gary G. and Tan, Kay Chen},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={A Survey on Evolutionary Neural Architecture Search}, 
  year={2021},
  volume={},
  number={},
  pages={1-21},
  doi={10.1109/TNNLS.2021.3100554}}

@InProceedings{24,
author = {Dong, Xuanyi and Yang, Yi},
title = {One-Shot Neural Architecture Search via Self-Evaluated Template Network},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019}
}

@InProceedings{25,
author = {Chen, Yukang and Meng, Gaofeng and Zhang, Qian and Xiang, Shiming and Huang, Chang and Mu, Lisen and Wang, Xinggang},
title = {RENAS: Reinforced Evolutionary Neural Architecture Search},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@misc{26,
  doi = {10.48550/ARXIV.2009.10199},
  url = {https://arxiv.org/abs/2009.10199},
  author = {Shi, Min and Wilson, David A. and Zhu, Xingquan and Huang, Yu and Zhuang, Yuan and Liu, Jianxun and Tang, Yufei},
  keywords = {Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Evolutionary Architecture Search for Graph Neural Networks},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{27,
  title={Graph Neural Architecture Search.},
  author={Gao, Yang and Yang, Hong and Zhang, Peng and Zhou, Chuan and Hu, Yue},
  booktitle={IJCAI},
  volume={20},
  pages={1403--1409},
  year={2020}
}

@misc{28,
  doi = {10.48550/ARXIV.1909.03184},
  url = {https://arxiv.org/abs/1909.03184},
  author = {Zhou, Kaixiong and Song, Qingquan and Huang, Xiao and Hu, Xia},
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Auto-GNN: Neural Architecture Search of Graph Neural Networks},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{29,
  title={Neural architecture search: A survey},
  author={Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
  journal={The Journal of Machine Learning Research},
  volume={20},
  number={1},
  pages={1997--2017},
  year={2019},
  publisher={JMLR. org}
}



@misc{30,
  doi = {10.48550/ARXIV.1904.09981},
  url = {https://arxiv.org/abs/1904.09981},
  author = {Gao, Yang and Yang, Hong and Zhang, Peng and Zhou, Chuan and Hu, Yue},
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {GraphNAS: Graph Neural Architecture Search with Reinforcement Learning},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@Article{31,
author={Khan, Asifullah
and Sohail, Anabia
and Zahoora, Umme
and Qureshi, Aqsa Saeed},
title={A survey of the recent architectures of deep convolutional neural networks},
journal={Artificial Intelligence Review},
year={2020},
month={Dec},
day={01},
volume={53},
number={8},
pages={5455-5516},
abstract={Deep Convolutional Neural Network (CNN) is a special type of Neural Networks, which has shown exemplary performance on several competitions related to Computer Vision and Image Processing. Some of the exciting application areas of CNN include Image Classification and Segmentation, Object Detection, Video Processing, Natural Language Processing, and Speech Recognition. The powerful learning ability of deep CNN is primarily due to the use of multiple feature extraction stages that can automatically learn representations from the data. The availability of a large amount of data and improvement in the hardware technology has accelerated the research in CNNs, and recently interesting deep CNN architectures have been reported. Several inspiring ideas to bring advancements in CNNs have been explored, such as the use of different activation and loss functions, parameter optimization, regularization, and architectural innovations. However, the significant improvement in the representational capacity of the deep CNN is achieved through architectural innovations. Notably, the ideas of exploiting spatial and channel information, depth and width of architecture, and multi-path information processing have gained substantial attention. Similarly, the idea of using a block of layers as a structural unit is also gaining popularity. This survey thus focuses on the intrinsic taxonomy present in the recently reported deep CNN architectures and, consequently, classifies the recent innovations in CNN architectures into seven different categories. These seven categories are based on spatial exploitation, depth, multi-path, width, feature-map exploitation, channel boosting, and attention. Additionally, the elementary understanding of CNN components, current challenges, and applications of CNN are also provided.},
issn={1573-7462},
doi={10.1007/s10462-020-09825-6},
url={https://doi.org/10.1007/s10462-020-09825-6}
}

@article{32,
  title={Evolutionary algorithms and neural networks},
  author={Mirjalili, Seyedali},
  journal={Studies in computational intelligence},
  volume={780},
  year={2019},
  publisher={Springer}
}

@article{33,
author = {Ashray Bhandare and Devinder Kaur},
doi = {doi:10.21307/ijanmc-2021-024},
url = {https://doi.org/10.21307/ijanmc-2021-024},
title = {Designing Convolutional Neural Network Architecture Using Genetic Algorithms},
journal = {International Journal of Advanced Network, Monitoring and Controls},
number = {3},
volume = {6},
year = {2021},
pages = {26--35}
}

@ARTICLE{34,
  author={Wang, Xinjie and Jin, Yaochu and Hao, Kuangrong},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Evolving Local Plasticity Rules for Synergistic Learning in Echo State Networks}, 
  year={2020},
  volume={31},
  number={4},
  pages={1363-1374},
  doi={10.1109/TNNLS.2019.2919903}}


@ARTICLE{35,
  author={Sun, Yanan and Wang, Handing and Xue, Bing and Jin, Yaochu and Yen, Gary G. and Zhang, Mengjie},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={Surrogate-Assisted Evolutionary Deep Learning Using an End-to-End Random Forest-Based Performance Predictor}, 
  year={2020},
  volume={24},
  number={2},
  pages={350-364},
  doi={10.1109/TEVC.2019.2924461}}


@ARTICLE{36,
  author={Sun, Yanan and Xue, Bing and Zhang, Mengjie and Yen, Gary G.},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Completely Automated CNN Architecture Design Based on Blocks}, 
  year={2020},
  volume={31},
  number={4},
  pages={1242-1254},
  doi={10.1109/TNNLS.2019.2919608}}

@ARTICLE{37,
  author={Ye, Qing and Sun, Yanan and Zhang, Jixin and Lv, Jiancheng},
  journal={IEEE Transactions on Parallel and Distributed Systems}, 
  title={A Distributed Framework for EA-Based NAS}, 
  year={2021},
  volume={32},
  number={7},
  pages={1753-1764},
  doi={10.1109/TPDS.2020.3046774}}

@ARTICLE{38,
  author={Sun, Yanan and Yen, Gary G. and Yi, Zhang},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={Improved Regularity Model-Based EDA for Many-Objective Optimization}, 
  year={2018},
  volume={22},
  number={5},
  pages={662-678},
  doi={10.1109/TEVC.2018.2794319}}

@inproceedings{39,
  title={Graphpas: Parallel architecture search for graph neural networks},
  author={Chen, Jiamin and Gao, Jianliang and Chen, Yibo and Oloulade, Moctard Babatounde and Lyu, Tengfei and Li, Zhao},
  booktitle={Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={2182--2186},
  year={2021}
}

@article{40,
	doi = {10.1561/2200000071},
	url = {https://doi.org/10.1561%2F2200000071},
	year = 2018,
	publisher = {Now Publishers},
	volume = {11},
	number = {3-4},
	pages = {219--354},
	author = {Vincent Fran{\c{c}}ois-Lavet and Peter Henderson and Riashat Islam and Marc G. Bellemare and Joelle Pineau},
	title = {An Introduction to Deep Reinforcement Learning},
	journal = {Foundations and Trends{\textregistered} in Machine Learning}
}

@INPROCEEDINGS{41,
  author={He, Qiang and Hou, Xinwen},
  booktitle={2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={WD3: Taming the Estimation Bias in Deep Reinforcement Learning}, 
  year={2020},
  volume={},
  number={},
  pages={391-398},
  doi={10.1109/ICTAI50040.2020.00068}}

@ARTICLE{42,
  author={Pan, Jie and Wang, Xuesong and Cheng, Yuhu and Yu, Qiang},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Multisource Transfer Double DQN Based on Actor Learning}, 
  year={2018},
  volume={29},
  number={6},
  pages={2227-2238},
  doi={10.1109/TNNLS.2018.2806087}}


@InProceedings{43,
  title = 	 {Addressing Function Approximation Error in Actor-Critic Methods},
  author =       {Fujimoto, Scott and van Hoof, Herke and Meger, David},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1587--1596},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/fujimoto18a/fujimoto18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/fujimoto18a.html},
  abstract = 	 {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.}
}


@misc{44,
  doi = {10.48550/ARXIV.1802.01725},
  url = {https://arxiv.org/abs/1802.01725},
  author = {Avatefipour, Omid and Malik, Hafiz}, 
  keywords = {Cryptography and Security (cs.CR), FOS: Computer and information sciences, FOS: Computer and information sciences}, 
  title = {State-of-the-Art Survey on In-Vehicle Network Communication (CAN-Bus) Security and Vulnerabilities},  
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@INPROCEEDINGS{45,
  author={Tashiro, Akiyoshi and Muraoka, Hideyuki and Araki, Shunsuke and Kakizaki, Ken'ichi and Uehara, Satoshi},
  booktitle={2017 3rd IEEE International Conference on Computer and Communications (ICCC)}, 
  title={A secure protocol consisting of two different security-level message authentications over CAN}, 
  year={2017},
  volume={},
  number={},
  pages={1520-1524},
  doi={10.1109/CompComm.2017.8322794}}

@INPROCEEDINGS{46,
  author={Nowdehi, Nasser and Lautenbach, Aljoscha and Olovsson, Tomas},
  booktitle={2017 IEEE 86th Vehicular Technology Conference (VTC-Fall)}, 
  title={In-Vehicle CAN Message Authentication: An Evaluation Based on Industrial Criteria}, 
  year={2017},
  volume={},
  number={},
  pages={1-7},
  doi={10.1109/VTCFall.2017.8288327}}

@ARTICLE{47,
  author={Choi, Wonsuk and Joo, Kyungho and Jo, Hyo Jin and Park, Moon Chan and Lee, Dong Hoon},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={VoltageIDS: Low-Level Communication Characteristics for Automotive Intrusion Detection System}, 
  year={2018},
  volume={13},
  number={8},
  pages={2114-2129},
  doi={10.1109/TIFS.2018.2812149}}

@article{48,
title = {In-vehicle network intrusion detection using deep convolutional neural network},
journal = {Vehicular Communications},
volume = {21},
pages = {100198},
year = {2020},
issn = {2214-2096},
doi = {https://doi.org/10.1016/j.vehcom.2019.100198},
url = {https://www.sciencedirect.com/science/article/pii/S2214209619302451},
author = {Hyun Min Song and Jiyoung Woo and Huy Kang Kim},
keywords = {In-vehicle network, Controller area network (CAN), Intrusion detection, Convolutional neural network (CNN)},
abstract = {The implementation of electronics in modern vehicles has resulted in an increase in attacks targeting in-vehicle networks; thus, attack detection models have caught the attention of the automotive industry and its researchers. Vehicle network security is an urgent and significant problem because the malfunctioning of vehicles can directly affect human and road safety. The controller area network (CAN), which is used as a de facto standard for in-vehicle networks, does not have sufficient security features, such as message encryption and sender authentication, to protect the network from cyber-attacks. In this paper, we propose an intrusion detection system (IDS) based on a deep convolutional neural network (DCNN) to protect the CAN bus of the vehicle. The DCNN learns the network traffic patterns and detects malicious traffic without hand-designed features. We designed the DCNN model, which was optimized for the data traffic of the CAN bus, to achieve high detection performance while reducing the unnecessary complexity in the architecture of the Inception-ResNet model. We performed an experimental study using the datasets we built with a real vehicle to evaluate our detection system. The experimental results demonstrate that the proposed IDS has significantly low false negative rates and error rates when compared to the conventional machine-learning algorithms.}
}

@INPROCEEDINGS{49,
  author={Taylor, Adrian and Leblanc, Sylvain and Japkowicz, Nathalie},
  booktitle={2016 IEEE International Conference on Data Science and Advanced Analytics (DSAA)}, 
  title={Anomaly Detection in Automobile Control Network Data with Long Short-Term Memory Networks}, 
  year={2016},
  volume={},
  number={},
  pages={130-139},
  doi={10.1109/DSAA.2016.20}}

@misc{50,
  doi = {10.48550/ARXIV.2006.02903},
  url = {https://arxiv.org/abs/2006.02903},
  author = {Ren, Pengzhen and Xiao, Yun and Chang, Xiaojun and Huang, Po-Yao and Li, Zhihui and Chen, Xiaojiang and Wang, Xin},
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {A Comprehensive Survey of Neural Architecture Search: Challenges and Solutions},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@InProceedings{51,
  title = 	 {{NAS}-Bench-101: Towards Reproducible Neural Architecture Search},
  author =       {Ying, Chris and Klein, Aaron and Christiansen, Eric and Real, Esteban and Murphy, Kevin and Hutter, Frank},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {7105--7114},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/ying19a/ying19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/ying19a.html},
  abstract = 	 {Recent advances in neural architecture search (NAS) demand tremendous computational resources, which makes it difficult to reproduce experiments and imposes a barrier-to-entry to researchers without access to large-scale computation. We aim to ameliorate these problems by introducing NAS-Bench-101, the first public architecture dataset for NAS research. To build NAS-Bench-101, we carefully constructed a compact, yet expressive, search space, exploiting graph isomorphisms to identify 423k unique convolutional architectures. We trained and evaluated all of these architectures multiple times on CIFAR-10 and compiled the results into a large dataset of over 5 million trained models. This allows researchers to evaluate the quality of a diverse range of models in milliseconds by querying the pre-computed dataset. We demonstrate its utility by analyzing the dataset as a whole and by benchmarking a range of architecture optimization algorithms.}
}

@ARTICLE{52,
  author={Sun, Yanan and Xue, Bing and Zhang, Mengjie and Yen, Gary G.},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={Evolving Deep Convolutional Neural Networks for Image Classification}, 
  year={2020},
  volume={24},
  number={2},
  pages={394-407},
  doi={10.1109/TEVC.2019.2916183}}

@INPROCEEDINGS{8790093,
  author={Irwin-Harris, William and Sun, Yanan and Xue, Bing and Zhang, Mengjie},
  booktitle={2019 IEEE Congress on Evolutionary Computation (CEC)}, 
  title={A Graph-Based Encoding for Evolutionary Convolutional Neural Network Architecture Design}, 
  year={2019},
  volume={},
  number={},
  pages={546-553},
  doi={10.1109/CEC.2019.8790093}}


@article{54, title={Regularized Evolution for Image Classifier Architecture Search}, volume={33}, url={https://ojs.aaai.org/index.php/AAAI/article/view/4405}, DOI={10.1609/aaai.v33i01.33014780}, abstractNote={&lt;p&gt;The effort devoted to hand-crafting neural network image classifiers has motivated the use of architecture search to discover them automatically. Although evolutionary algorithms have been repeatedly applied to neural network topologies, the image classifiers thus discovered have remained inferior to human-crafted ones. Here, we evolve an image classifier— &lt;em&gt;AmoebaNet-A&lt;/em&gt;—that surpasses hand-designs for the first time. To do this, we modify the tournament selection evolutionary algorithm by introducing an age property to favor the younger genotypes. Matching size, AmoebaNet-A has comparable accuracy to current state-of-the-art ImageNet models discovered with more complex architecture-search methods. Scaled to larger size, AmoebaNet-A sets a new state-of-theart 83.9% top-1 / 96.6% top-5 ImageNet accuracy. In a controlled comparison against a well known reinforcement learning algorithm, we give evidence that evolution can obtain results faster with the same hardware, especially at the earlier stages of the search. This is relevant when fewer compute resources are available. Evolution is, thus, a simple method to effectively discover high-quality architectures.&lt;/p&gt;}, number={01}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V.}, year={2019}, month={Jul.}, pages={4780-4789} }

@InProceedings{10.1007/978-3-030-61377-8_21,
author="Nunes, Matheus
and Pappa, Gisele L.",
editor="Cerri, Ricardo
and Prati, Ronaldo C.",
title="Neural Architecture Search in Graph Neural Networks",
booktitle="Intelligent Systems",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="302--317",
abstract="Performing analytical tasks over graph data has become increasingly interesting due to the ubiquity and large availability of relational information. However, unlike images or sentences, there is no notion of sequence in networks. Nodes (and edges) follow no absolute order, and it is hard for traditional machine learning (ML) algorithms to recognize a pattern and generalize their predictions on this type of data. Graph Neural Networks (GNN) successfully tackled this problem. They became popular after the generalization of the convolution concept to the graph domain. However, they possess a large number of hyper-parameters and their design and optimization is currently hand-made, based on heuristics or empirical intuition. Neural Architecture Search (NAS) methods appear as an interesting solution to this problem. In this direction, this paper compares two NAS methods for optimizing GNN: one based on reinforcement learning and a second based on evolutionary algorithms. Results consider 7 datasets over two search spaces and show that both methods obtain similar accuracies to a random search, raising the question of how many of the search space dimensions are actually relevant to the problem.",
isbn="978-3-030-61377-8"
}


@ARTICLE{55,
  author={Wang, Chaoyue and Xu, Chang and Yao, Xin and Tao, Dacheng},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={Evolutionary Generative Adversarial Networks}, 
  year={2019},
  volume={23},
  number={6},
  pages={921-934},
  doi={10.1109/TEVC.2019.2895748}}

@INPROCEEDINGS{56,
  author={Yin, Zixuan and Gross, Warren and Meyer, Brett H.},
  booktitle={2020 Design, Automation   Test in Europe Conference   Exhibition (DATE)}, 
  title={Probabilistic Sequential Multi-Objective Optimization of Convolutional Neural Networks}, 
  year={2020},
  volume={},
  number={},
  pages={1055-1060},
  doi={10.23919/DATE48585.2020.9116535}}

@misc{57,
  doi = {10.48550/ARXIV.1804.09081},
  url = {https://arxiv.org/abs/1804.09081},
  author = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Efficient Multi-objective Neural Architecture Search via Lamarckian Evolution},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{58,
  title={Nsga-net: neural architecture search using multi-objective genetic algorithm},
  author={Lu, Zhichao and Whalen, Ian and Boddeti, Vishnu and Dhebar, Yashesh and Deb, Kalyanmoy and Goodman, Erik and Banzhaf, Wolfgang},
  booktitle={Proceedings of the genetic and evolutionary computation conference},
  pages={419--427},
  year={2019}
}



@ARTICLE{59,
  author={Sun, Yanan and Xue, Bing and Zhang, Mengjie and Yen, Gary G. and Lv, Jiancheng},
  journal={IEEE Transactions on Cybernetics}, 
  title={Automatically Designing CNN Architectures Using the Genetic Algorithm for Image Classification}, 
  year={2020},
  volume={50},
  number={9},
  pages={3840-3854},
  doi={10.1109/TCYB.2020.2983860}}

@InProceedings{60,
author = {Xie, Lingxi and Yuille, Alan},
title = {Genetic CNN},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {Oct},
year = {2017}
}


@InProceedings{61,
  title = 	 {Large-Scale Evolution of Image Classifiers},
  author =       {Esteban Real and Sherry Moore and Andrew Selle and Saurabh Saxena and Yutaka Leon Suematsu and Jie Tan and Quoc V. Le and Alexey Kurakin},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {2902--2911},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/real17a/real17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/real17a.html},
  abstract = 	 {Neural networks have proven effective at solving difficult problems but designing their architectures can be challenging, even for image classification problems alone. Our goal is to minimize human participation, so we employ evolutionary algorithms to discover such networks automatically. Despite significant computational requirements, we show that it is now possible to evolve models with accuracies within the range of those published in the last year. Specifically, we employ simple evolutionary techniques at unprecedented scales to discover models for the CIFAR-10 and CIFAR-100 datasets, starting from trivial initial conditions and reaching accuracies of 94.6\% (95.6\% for ensemble) and 77.0\%, respectively. To do this, we use novel and intuitive mutation operators that navigate large search spaces; we stress that no human participation is required once evolution starts and that the output is a fully-trained model. Throughout this work, we place special emphasis on the repeatability of results, the variability in the outcomes and the computational requirements.}
}

@InProceedings{62,
author="Lu, Zhichao
and Deb, Kalyanmoy
and Goodman, Erik
and Banzhaf, Wolfgang
and Boddeti, Vishnu Naresh",
editor="Vedaldi, Andrea
and Bischof, Horst
and Brox, Thomas
and Frahm, Jan-Michael",
title="NSGANetV2: Evolutionary Multi-objective Surrogate-Assisted Neural Architecture Search",
booktitle="Computer Vision -- ECCV 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="35--51",
abstract="In this paper, we propose an efficient NAS algorithm for generating task-specific models that are competitive under multiple competing objectives. It comprises of two surrogates, one at the architecture level to improve sample efficiency and one at the weights level, through a supernet, to improve gradient descent training efficiency. On standard benchmark datasets (C10, C100, ImageNet), the resulting models, dubbed NSGANetV2, either match or outperform models from existing approaches with the search being orders of magnitude more sample efficient. Furthermore, we demonstrate the effectiveness and versatility of the proposed method on six diverse non-standard datasets, e.g. STL-10, Flowers102, Oxford Pets, FGVC Aircrafts etc. In all cases, NSGANetV2s improve the state-of-the-art (under mobile setting), suggesting that NAS can be a viable alternative to conventional transfer learning approaches in handling diverse scenarios such as small-scale or fine-grained datasets. Code is available at https://github.com/mikelzc1990/nsganetv2.",
isbn="978-3-030-58452-8"
}

@misc{63,
  doi = {10.48550/ARXIV.1705.10823},
  url = {https://arxiv.org/abs/1705.10823},
  author = {Baker, Bowen and Gupta, Otkrist and Raskar, Ramesh and Naik, Nikhil},
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Accelerating Neural Architecture Search using Performance Prediction},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@InProceedings{64,
author = {Liu, Chenxi and Zoph, Barret and Neumann, Maxim and Shlens, Jonathon and Hua, Wei and Li, Li-Jia and Fei-Fei, Li and Yuille, Alan and Huang, Jonathan and Murphy, Kevin},
title = {Progressive Neural Architecture Search},
booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
month = {September},
year = {2018}
}

@misc{65,
  doi = {10.48550/ARXIV.1908.09791},
  url = {https://arxiv.org/abs/1908.09791},
  author = {Cai, Han and Gan, Chuang and Wang, Tianzhe and Zhang, Zhekai and Han, Song},
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Once-for-All: Train One Network and Specialize it for Efficient Deployment},
  publisher = {arXiv},
  year = {2019},
  copyright = {Creative Commons Attribution 4.0 International}
}

@InProceedings{66,
author = {Dai, Xiaoliang and Zhang, Peizhao and Wu, Bichen and Yin, Hongxu and Sun, Fei and Wang, Yanghan and Dukhan, Marat and Hu, Yunqing and Wu, Yiming and Jia, Yangqing and Vajda, Peter and Uyttendaele, Matt and Jha, Niraj K.},
title = {ChamNet: Towards Efficient Network Design Through Platform-Aware Model Adaptation},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@InProceedings{67,
author = {Yu, Jiahui and Huang, Thomas S.},
title = {Universally Slimmable Networks and Improved Training Techniques},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019}
}

@misc{68,
  doi = {10.48550/ARXIV.1611.01578},
  url = {https://arxiv.org/abs/1611.01578},
  author = {Zoph, Barret and Le, Quoc V.},
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Neural Architecture Search with Reinforcement Learning},
  publisher = {arXiv},
  year = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{69,
  doi = {10.48550/ARXIV.1912.12814},
  url = {https://arxiv.org/abs/1912.12814},
  author = {Jin, Xiaojie and Wang, Jiang and Slocum, Joshua and Yang, Ming-Hsuan and Dai, Shengyang and Yan, Shuicheng and Feng, Jiashi},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {RC-DARTS: Resource Constrained Differentiable Architecture Search},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@InProceedings{70,
author = {Zheng, Xiawu and Ji, Rongrong and Tang, Lang and Zhang, Baochang and Liu, Jianzhuang and Tian, Qi},
title = {Multinomial Distribution Learning for Effective Neural Architecture Search},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019}
}

@InProceedings{71,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
title = {Deep Residual Learning for Image Recognition},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2016}
}

@inproceedings{72,
  title={Inception-v4, inception-resnet and the impact of residual connections on learning},
  author={Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A},
  booktitle={Thirty-first AAAI conference on artificial intelligence},
  year={2017}
}

@InProceedings{73,
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
title = {Rethinking the Inception Architecture for Computer Vision},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2016}
}

@data{74,
doi = {10.21227/qvr7-n418},
url = {https://dx.doi.org/10.21227/qvr7-n418},
author = {Kang, Hyunjae and Kwak, Byung Il and Lee, Young Hun and Lee, Haneol and Lee, Hwejae and Kim, Huy Kang},
publisher = {IEEE Dataport},
title = {Car Hacking: Attack $\&$ Defense Challenge 2020 Dataset},
year = {2021} }



@INPROCEEDINGS{75,
  author={Kang, Min-Ju and Kang, Je-Won},
  booktitle={2016 IEEE 83rd Vehicular Technology Conference (VTC Spring)}, 
  title={A Novel Intrusion Detection Method Using Deep Neural Network for In-Vehicle Network Security}, 
  year={2016},
  volume={},
  number={},
  pages={1-5},
  doi={10.1109/VTCSpring.2016.7504089}}










